## Command Line Interface

<span id="cli" />

The `corp-extractor` CLI provides three commands for different use cases.

### Commands Overview

<span id="commands-overview" />

<table>
  <thead>
    <tr>
      <th>Command</th>
      <th>Description</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>split</code></td>
      <td>Simple extraction (Stage 1 only)</td>
      <td>Fast extraction, basic triples</td>
    </tr>
    <tr>
      <td><code>pipeline</code></td>
      <td>Full 6-stage pipeline</td>
      <td>Entity resolution, canonicalization, labeling, taxonomy</td>
    </tr>
    <tr>
      <td><code>plugins</code></td>
      <td>Plugin management</td>
      <td>List and inspect available plugins</td>
    </tr>
  </tbody>
</table>

### Installation

<span id="cli-installation" />

For best results, install globally:

```bash
# Using uv (recommended)
uv tool install "corp-extractor[embeddings]"

# Using pipx
pipx install "corp-extractor[embeddings]"

# Using pip
pip install "corp-extractor[embeddings]"
```

### Quick Run with uvx

<span id="uvx" />

Run directly without installing using [uv](https://docs.astral.sh/uv/):

```bash
uvx corp-extractor split "Apple announced a new iPhone."
```

**Note**: First run downloads the model (~1.5GB) which may take a few minutes.

---

### Split Command

<span id="split-command" />

The `split` command extracts sub-statements using the T5-Gemma model. It's fast and simple—use `pipeline` for full entity resolution.

```bash
# Extract from text argument
corp-extractor split "Apple Inc. announced the iPhone 15."

# Extract from file
corp-extractor split -f article.txt

# Pipe from stdin
cat article.txt | corp-extractor split -

# Output as JSON
corp-extractor split "Tim Cook is CEO of Apple." --json

# Output as XML
corp-extractor split "Tim Cook is CEO of Apple." --xml

# Verbose output with confidence scores
corp-extractor split -f article.txt --verbose

# Use more beams for better quality
corp-extractor split -f article.txt --beams 8
```

#### Split Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>-f, --file PATH</code></td>
      <td>Read input from file</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>-o, --output</code></td>
      <td>Output format: table, json, xml</td>
      <td>table</td>
    </tr>
    <tr>
      <td><code>--json / --xml</code></td>
      <td>Output format shortcuts</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>-b, --beams</code></td>
      <td>Number of beams for diverse beam search</td>
      <td>4</td>
    </tr>
    <tr>
      <td><code>--diversity</code></td>
      <td>Diversity penalty for beam search</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td><code>--no-gliner</code></td>
      <td>Disable GLiNER2 extraction</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--predicates</code></td>
      <td>Comma-separated predicates for relation extraction</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--predicates-file</code></td>
      <td>Path to custom predicates JSON file</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--device</code></td>
      <td>Device: auto, cuda, mps, cpu</td>
      <td>auto</td>
    </tr>
    <tr>
      <td><code>-v, --verbose</code></td>
      <td>Show confidence scores and metadata</td>
      <td>—</td>
    </tr>
  </tbody>
</table>

---

### Pipeline Command

<span id="pipeline-command" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.5.0</span>

The `pipeline` command runs the full 6-stage extraction pipeline for comprehensive entity resolution and taxonomy classification.

```bash
# Run all 6 stages
corp-extractor pipeline "Amazon CEO Andy Jassy announced plans to hire workers."

# Run from file
corp-extractor pipeline -f article.txt

# Run specific stages
corp-extractor pipeline "..." --stages 1-3
corp-extractor pipeline "..." --stages 1,2,5

# Skip specific stages
corp-extractor pipeline "..." --skip-stages 4,5

# Enable specific plugins only
corp-extractor pipeline "..." --plugins gleif,companies_house

# Disable specific plugins
corp-extractor pipeline "..." --disable-plugins sec_edgar

# Output formats
corp-extractor pipeline "..." -o json
corp-extractor pipeline "..." -o yaml
corp-extractor pipeline "..." -o triples
```

#### Pipeline Stages

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Name</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Splitting</td>
      <td>Text → Raw triples (T5-Gemma)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Extraction</td>
      <td>Raw triples → Typed statements (GLiNER2)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Qualification</td>
      <td>Add qualifiers and identifiers (roles, LEI, etc.)</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Canonicalization</td>
      <td>Resolve to canonical forms</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Labeling</td>
      <td>Apply sentiment, relation type, confidence</td>
    </tr>
    <tr>
      <td>6</td>
      <td>Taxonomy</td>
      <td>Classify against large taxonomies (MNLI/embeddings)</td>
    </tr>
  </tbody>
</table>

#### Pipeline Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--stages</code></td>
      <td>Stages to run</td>
      <td><code>1-3</code> or <code>1,2,5</code></td>
    </tr>
    <tr>
      <td><code>--skip-stages</code></td>
      <td>Stages to skip</td>
      <td><code>4,5</code></td>
    </tr>
    <tr>
      <td><code>--plugins</code></td>
      <td>Enable only these plugins</td>
      <td><code>gleif,person</code></td>
    </tr>
    <tr>
      <td><code>--disable-plugins</code></td>
      <td>Disable these plugins</td>
      <td><code>sec_edgar</code></td>
    </tr>
    <tr>
      <td><code>--predicates-file</code></td>
      <td>Custom predicates JSON file for GLiNER2</td>
      <td><code>custom.json</code></td>
    </tr>
    <tr>
      <td><code>-o, --output</code></td>
      <td>Output format</td>
      <td>table, json, yaml, triples</td>
    </tr>
  </tbody>
</table>

---

### Plugins Command

<span id="plugins-command" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.5.0</span>

The `plugins` command lists and inspects available pipeline plugins.

```bash
# List all plugins
corp-extractor plugins list

# List plugins for a specific stage
corp-extractor plugins list --stage 3

# Get details about a plugin
corp-extractor plugins info gleif_qualifier
corp-extractor plugins info person_qualifier
```

**Example output:**

```
Stage 1: Splitting
----------------------------------------
  t5_gemma_splitter  [priority: 100]

Stage 2: Extraction
----------------------------------------
  gliner2_extractor  [priority: 100]

Stage 3: Qualification
----------------------------------------
  person_qualifier (PERSON)  [priority: 100]
  gleif_qualifier (ORG)  [priority: 90]
  companies_house_qualifier (ORG)  [priority: 80]
  sec_edgar_qualifier (ORG)  [priority: 70]

Stage 4: Canonicalization
----------------------------------------
  organization_canonicalizer (ORG)  [priority: 100]
  person_canonicalizer (PERSON)  [priority: 100]

Stage 5: Labeling
----------------------------------------
  sentiment_labeler  [priority: 100]
```

---

### Output Formats

<span id="cli-output" />

**Table output (default):**
```
Extracted 2 statement(s):

--------------------------------------------------------------------------------
1. Andy Jassy (CEO, Amazon)
   --[announced]-->
   plans to hire workers
--------------------------------------------------------------------------------
```

**JSON output:**
```json
{
  "statement_count": 2,
  "labeled_statements": [
    {
      "subject": {"text": "Andy Jassy", "type": "PERSON", "fqn": "Andy Jassy (CEO, Amazon)"},
      "predicate": "announced",
      "object": {"text": "plans to hire workers", "type": "EVENT"},
      "labels": {"sentiment": "positive"}
    }
  ]
}
```

**Triples output:**
```
Andy Jassy (CEO, Amazon)	announced	plans to hire workers
Amazon	has CEO	Andy Jassy (CEO, Amazon)
```

---

### Shell Integration

<span id="cli-shell" />

**Processing multiple files:**

```bash
# Process all .txt files
for f in *.txt; do
  echo "=== $f ==="
  corp-extractor pipeline -f "$f" -o json > "${f%.txt}.json"
done
```

**Combining with jq:**

```bash
# Extract just predicates
corp-extractor split "Your text" --json | jq '.statements[].predicate'

# Filter high-confidence statements
corp-extractor split -f article.txt --json | jq '.statements[] | select(.confidence_score > 0.8)'

# Get FQNs from pipeline
corp-extractor pipeline "Your text" -o json | jq '.labeled_statements[].subject.fqn'
```

---

### Database Commands

<span id="db-commands" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.6.0</span>

The `db` command group manages the company embedding database used for entity qualification.

```bash
# Show database status
corp-extractor db status

# Search for a company
corp-extractor db search "Microsoft"
corp-extractor db search "Barclays" --source companies_house

# Import from data sources
corp-extractor db import-gleif --download
corp-extractor db import-sec
corp-extractor db import-companies-house --download
corp-extractor db import-wikidata --limit 50000

# Download/upload from HuggingFace Hub
corp-extractor db download
corp-extractor db upload companies.db
```

#### Data Sources

<table>
  <thead>
    <tr>
      <th>Source</th>
      <th>Command</th>
      <th>Records</th>
      <th>Identifier</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GLEIF</td>
      <td><code>import-gleif --download</code></td>
      <td>~3.2M</td>
      <td>LEI</td>
    </tr>
    <tr>
      <td>SEC Edgar</td>
      <td><code>import-sec</code></td>
      <td>~10K</td>
      <td>CIK</td>
    </tr>
    <tr>
      <td>Companies House</td>
      <td><code>import-companies-house --download</code></td>
      <td>~5M</td>
      <td>Company Number</td>
    </tr>
    <tr>
      <td>Wikidata</td>
      <td><code>import-wikidata</code></td>
      <td>Variable</td>
      <td>QID</td>
    </tr>
  </tbody>
</table>

#### Database Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--db PATH</code></td>
      <td>Database file path</td>
      <td>~/.cache/corp-extractor/companies.db</td>
    </tr>
    <tr>
      <td><code>--limit N</code></td>
      <td>Limit number of records</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--download</code></td>
      <td>Download source data automatically</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--force</code></td>
      <td>Force re-download even if cached</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--source</code></td>
      <td>Filter search by source</td>
      <td>—</td>
    </tr>
  </tbody>
</table>

See [COMPANY_DB.md](https://github.com/corp-o-rate/statement-extractor/blob/main/COMPANY_DB.md) for complete build and publish instructions.
