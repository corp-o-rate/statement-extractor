## Command Line Interface

<span id="cli" />

The `corp-extractor` CLI provides commands for extraction, document processing, and database management.

### Commands Overview

<span id="commands-overview" />

<table>
  <thead>
    <tr>
      <th>Command</th>
      <th>Description</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>split</code></td>
      <td>Simple extraction (Stage 1 only)</td>
      <td>Fast extraction, basic triples</td>
    </tr>
    <tr>
      <td><code>pipeline</code></td>
      <td>Full 5-stage pipeline</td>
      <td>Entity resolution, labeling, taxonomy</td>
    </tr>
    <tr>
      <td><code>document</code></td>
      <td>Document processing</td>
      <td>Files, URLs, PDFs with chunking and deduplication</td>
    </tr>
    <tr>
      <td><code>db</code></td>
      <td>Database management</td>
      <td>Import, search, upload/download entity database</td>
    </tr>
    <tr>
      <td><code>plugins</code></td>
      <td>Plugin management</td>
      <td>List and inspect available plugins</td>
    </tr>
  </tbody>
</table>

### Installation

<span id="cli-installation" />

For best results, install globally:

```bash
# Using uv (recommended)
uv tool install "corp-extractor[embeddings]"

# Using pipx
pipx install "corp-extractor[embeddings]"

# Using pip
pip install "corp-extractor[embeddings]"
```

### Quick Run with uvx

<span id="uvx" />

Run directly without installing using [uv](https://docs.astral.sh/uv/):

```bash
uvx corp-extractor split "Apple announced a new iPhone."
```

**Note**: First run downloads the model (~1.5GB) which may take a few minutes.

---

### Split Command

<span id="split-command" />

The `split` command extracts sub-statements using the T5-Gemma model. It's fast and simple—use `pipeline` for full entity resolution.

```bash
# Extract from text argument
corp-extractor split "Apple Inc. announced the iPhone 15."

# Extract from file
corp-extractor split -f article.txt

# Pipe from stdin
cat article.txt | corp-extractor split -

# Output as JSON
corp-extractor split "Tim Cook is CEO of Apple." --json

# Output as XML
corp-extractor split "Tim Cook is CEO of Apple." --xml

# Verbose output with confidence scores
corp-extractor split -f article.txt --verbose

# Use more beams for better quality
corp-extractor split -f article.txt --beams 8
```

#### Split Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>-f, --file PATH</code></td>
      <td>Read input from file</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>-o, --output</code></td>
      <td>Output format: table, json, xml</td>
      <td>table</td>
    </tr>
    <tr>
      <td><code>--json / --xml</code></td>
      <td>Output format shortcuts</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>-b, --beams</code></td>
      <td>Number of beams for diverse beam search</td>
      <td>4</td>
    </tr>
    <tr>
      <td><code>--diversity</code></td>
      <td>Diversity penalty for beam search</td>
      <td>1.0</td>
    </tr>
    <tr>
      <td><code>--no-gliner</code></td>
      <td>Disable GLiNER2 extraction</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--predicates</code></td>
      <td>Comma-separated predicates for relation extraction</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--predicates-file</code></td>
      <td>Path to custom predicates JSON file</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--device</code></td>
      <td>Device: auto, cuda, mps, cpu</td>
      <td>auto</td>
    </tr>
    <tr>
      <td><code>-v, --verbose</code></td>
      <td>Show confidence scores and metadata</td>
      <td>—</td>
    </tr>
  </tbody>
</table>

---

### Pipeline Command

<span id="pipeline-command" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.5.0</span>

The `pipeline` command runs the full 5-stage extraction pipeline for comprehensive entity resolution and taxonomy classification.

```bash
# Run all 5 stages
corp-extractor pipeline "Amazon CEO Andy Jassy announced plans to hire workers."

# Run from file
corp-extractor pipeline -f article.txt

# Run specific stages
corp-extractor pipeline "..." --stages 1-3
corp-extractor pipeline "..." --stages 1,2,5

# Skip specific stages
corp-extractor pipeline "..." --skip-stages 4,5

# Enable specific plugins only
corp-extractor pipeline "..." --plugins gleif,companies_house

# Disable specific plugins
corp-extractor pipeline "..." --disable-plugins sec_edgar

# Output formats
corp-extractor pipeline "..." -o json
corp-extractor pipeline "..." -o yaml
corp-extractor pipeline "..." -o triples
```

#### Pipeline Stages

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Name</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>Splitting</td>
      <td>Text → Raw triples (T5-Gemma)</td>
    </tr>
    <tr>
      <td>2</td>
      <td>Extraction</td>
      <td>Raw triples → Typed statements (GLiNER2)</td>
    </tr>
    <tr>
      <td>3</td>
      <td>Entity Qualification</td>
      <td>Add identifiers (LEI, CIK, etc.) and canonical names via embedding DB</td>
    </tr>
    <tr>
      <td>4</td>
      <td>Labeling</td>
      <td>Apply sentiment, relation type, confidence</td>
    </tr>
    <tr>
      <td>5</td>
      <td>Taxonomy</td>
      <td>Classify against large taxonomies (MNLI/embeddings)</td>
    </tr>
  </tbody>
</table>

#### Pipeline Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--stages</code></td>
      <td>Stages to run</td>
      <td><code>1-3</code> or <code>1,2,5</code></td>
    </tr>
    <tr>
      <td><code>--skip-stages</code></td>
      <td>Stages to skip</td>
      <td><code>4,5</code></td>
    </tr>
    <tr>
      <td><code>--plugins</code></td>
      <td>Enable only these plugins</td>
      <td><code>gleif,person</code></td>
    </tr>
    <tr>
      <td><code>--disable-plugins</code></td>
      <td>Disable these plugins</td>
      <td><code>sec_edgar</code></td>
    </tr>
    <tr>
      <td><code>--predicates-file</code></td>
      <td>Custom predicates JSON file for GLiNER2</td>
      <td><code>custom.json</code></td>
    </tr>
    <tr>
      <td><code>-o, --output</code></td>
      <td>Output format</td>
      <td>table, json, yaml, triples</td>
    </tr>
  </tbody>
</table>

---

### Plugins Command

<span id="plugins-command" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.5.0</span>

The `plugins` command lists and inspects available pipeline plugins.

```bash
# List all plugins
corp-extractor plugins list

# List plugins for a specific stage
corp-extractor plugins list --stage 3

# Get details about a plugin
corp-extractor plugins info gleif_qualifier
corp-extractor plugins info person_qualifier
```

**Example output:**

```
Stage 1: Splitting
----------------------------------------
  t5_gemma_splitter  [priority: 100]

Stage 2: Extraction
----------------------------------------
  gliner2_extractor  [priority: 100]

Stage 3: Entity Qualification
----------------------------------------
  person_qualifier (PERSON)  [priority: 100]
  embedding_company_qualifier (ORG)  [priority: 5]

Stage 4: Labeling
----------------------------------------
  sentiment_labeler  [priority: 100]
  confidence_labeler  [priority: 100]
  relation_type_labeler  [priority: 100]

Stage 5: Taxonomy
----------------------------------------
  embedding_taxonomy_classifier  [priority: 100]
```

---

### Output Formats

<span id="cli-output" />

**Table output (default):**
```
Extracted 2 statement(s):

--------------------------------------------------------------------------------
1. Andy Jassy (CEO, Amazon)
   --[announced]-->
   plans to hire workers
--------------------------------------------------------------------------------
```

**JSON output:**
```json
{
  "statement_count": 2,
  "labeled_statements": [
    {
      "subject": {"text": "Andy Jassy", "type": "PERSON", "fqn": "Andy Jassy (CEO, Amazon)"},
      "predicate": "announced",
      "object": {"text": "plans to hire workers", "type": "EVENT"},
      "labels": {"sentiment": "positive"}
    }
  ]
}
```

**Triples output:**
```
Andy Jassy (CEO, Amazon)	announced	plans to hire workers
Amazon	has CEO	Andy Jassy (CEO, Amazon)
```

---

### Shell Integration

<span id="cli-shell" />

**Processing multiple files:**

```bash
# Process all .txt files
for f in *.txt; do
  echo "=== $f ==="
  corp-extractor pipeline -f "$f" -o json > "${f%.txt}.json"
done
```

**Combining with jq:**

```bash
# Extract just predicates
corp-extractor split "Your text" --json | jq '.statements[].predicate'

# Filter high-confidence statements
corp-extractor split -f article.txt --json | jq '.statements[] | select(.confidence_score > 0.8)'

# Get FQNs from pipeline
corp-extractor pipeline "Your text" -o json | jq '.labeled_statements[].subject.fqn'
```

---

### Document Command

<span id="document-commands" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.7.0</span>

The `document` command processes files, URLs, and PDFs with automatic chunking and deduplication.

```bash
# Process local files
corp-extractor document process article.txt
corp-extractor document process report.txt --title "Annual Report" --year 2024

# Process URLs (web pages and PDFs)
corp-extractor document process https://example.com/article
corp-extractor document process https://example.com/report.pdf --use-ocr

# Configure chunking
corp-extractor document process article.txt --max-tokens 500 --overlap 50

# Preview chunking without extraction
corp-extractor document chunk article.txt --max-tokens 500

# Output formats
corp-extractor document process article.txt -o json
corp-extractor document process article.txt -o triples
```

#### Document Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--title</code></td>
      <td>Document title for citations</td>
      <td>Filename</td>
    </tr>
    <tr>
      <td><code>--max-tokens</code></td>
      <td>Target tokens per chunk</td>
      <td>1000</td>
    </tr>
    <tr>
      <td><code>--overlap</code></td>
      <td>Token overlap between chunks</td>
      <td>100</td>
    </tr>
    <tr>
      <td><code>--use-ocr</code></td>
      <td>Force OCR for PDF parsing</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--no-summary</code></td>
      <td>Skip document summarization</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--no-dedup</code></td>
      <td>Skip cross-chunk deduplication</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--stages</code></td>
      <td>Pipeline stages to run</td>
      <td>1-5</td>
    </tr>
  </tbody>
</table>

---

### Database Commands

<span id="db-commands" />

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">UPDATED in v0.9.4</span>

The `db` command group manages the entity embedding database used for organization, person, role, and location qualification.

```bash
# Show database status
corp-extractor db status

# Search for an organization
corp-extractor db search "Microsoft"
corp-extractor db search "Barclays" --source companies_house

# Search for a person (v0.9.0)
corp-extractor db search-people "Tim Cook"
corp-extractor db search-people "Elon Musk" --top-k 5

# Search for roles (v0.9.4)
corp-extractor db search-roles "CEO"
corp-extractor db search-roles "Chief Financial Officer"

# Search for locations (v0.9.4)
corp-extractor db search-locations "California"
corp-extractor db search-locations "Germany" --type country

# Import organizations from data sources
corp-extractor db import-gleif --download
corp-extractor db import-sec --download           # Bulk data (~100K+ filers)
corp-extractor db import-companies-house --download
corp-extractor db import-wikidata --limit 50000   # SPARQL-based

# Import notable people (v0.9.0)
corp-extractor db import-people --type executive --limit 5000
corp-extractor db import-people --all --limit 10000  # All person types

# Import from Wikidata dump (v0.9.1) - avoids SPARQL timeouts
corp-extractor db import-wikidata-dump --download --limit 50000
corp-extractor db import-wikidata-dump --dump /path/to/dump.bz2 --people --no-orgs

# Download/upload from HuggingFace Hub
corp-extractor db download                        # Lite version (default)
corp-extractor db download --full                 # Full version with metadata
corp-extractor db upload                          # Upload with all variants

# Migrate from old schema (companies.db → entities.db)
corp-extractor db migrate companies.db --rename-file

# Migrate to v2 normalized schema (v0.9.4)
corp-extractor db migrate-v2 entities.db entities-v2.db
corp-extractor db migrate-v2 entities.db entities-v2.db --resume  # Resume interrupted

# Generate int8 scalar embeddings (v0.9.4) - 75% smaller
corp-extractor db backfill-scalar
corp-extractor db backfill-scalar --skip-generate  # Only quantize existing

# Local database management
corp-extractor db create-lite entities.db         # Create lite version
corp-extractor db compress entities.db            # Compress with gzip
```

#### Organization Data Sources

<table>
  <thead>
    <tr>
      <th>Source</th>
      <th>Command</th>
      <th>Records</th>
      <th>Identifier</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GLEIF</td>
      <td><code>import-gleif --download</code></td>
      <td>~3.2M</td>
      <td>LEI</td>
    </tr>
    <tr>
      <td>SEC Edgar</td>
      <td><code>import-sec --download</code></td>
      <td>~100K+</td>
      <td>CIK</td>
    </tr>
    <tr>
      <td>Companies House</td>
      <td><code>import-companies-house --download</code></td>
      <td>~5M</td>
      <td>Company Number</td>
    </tr>
    <tr>
      <td>Wikidata (SPARQL)</td>
      <td><code>import-wikidata</code></td>
      <td>Variable</td>
      <td>QID</td>
    </tr>
    <tr>
      <td>Wikidata (Dump)</td>
      <td><code>import-wikidata-dump --download</code></td>
      <td>All with enwiki</td>
      <td>QID</td>
    </tr>
  </tbody>
</table>

#### Person Data Sources <span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.0</span>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Command</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Executives</td>
      <td><code>import-people --type executive</code></td>
      <td>CEOs, CFOs, board members</td>
    </tr>
    <tr>
      <td>Politicians</td>
      <td><code>import-people --type politician</code></td>
      <td>Elected officials, diplomats</td>
    </tr>
    <tr>
      <td>Athletes</td>
      <td><code>import-people --type athlete</code></td>
      <td>Sports figures, coaches</td>
    </tr>
    <tr>
      <td>Artists</td>
      <td><code>import-people --type artist</code></td>
      <td>Actors, musicians, directors</td>
    </tr>
    <tr>
      <td>All Types</td>
      <td><code>import-people --all</code></td>
      <td>Run all person type queries</td>
    </tr>
  </tbody>
</table>

#### Person Import Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--skip-existing</code></td>
      <td>Skip existing records instead of updating them</td>
    </tr>
    <tr>
      <td><code>--enrich-dates</code></td>
      <td>Query individual records for start/end dates (slower)</td>
    </tr>
  </tbody>
</table>

#### Wikidata Dump Import <span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.1</span>

For large imports that avoid SPARQL timeouts, use the Wikidata JSON dump:

```bash
# Download and import (~100GB dump file)
corp-extractor db import-wikidata-dump --download --limit 50000

# Import only people
corp-extractor db import-wikidata-dump --download --people --no-orgs --limit 100000

# Import only organizations
corp-extractor db import-wikidata-dump --download --orgs --no-people --limit 100000

# Import only locations (v0.9.4)
corp-extractor db import-wikidata-dump --dump dump.bz2 --locations --no-people --no-orgs

# Use existing dump file
corp-extractor db import-wikidata-dump --dump /path/to/latest-all.json.bz2
```

**Fast download with aria2c:** Install `aria2c` for 10-20x faster downloads:
```bash
brew install aria2   # macOS
apt install aria2    # Ubuntu/Debian
```

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--download</code></td>
      <td>Download the Wikidata dump (~100GB)</td>
    </tr>
    <tr>
      <td><code>--dump PATH</code></td>
      <td>Use existing dump file (.bz2 or .gz)</td>
    </tr>
    <tr>
      <td><code>--people/--no-people</code></td>
      <td>Import people (default: yes)</td>
    </tr>
    <tr>
      <td><code>--orgs/--no-orgs</code></td>
      <td>Import organizations (default: yes)</td>
    </tr>
    <tr>
      <td><code>--locations/--no-locations</code></td>
      <td>Import locations (default: no) <span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.4</span></td>
    </tr>
    <tr>
      <td><code>--no-aria2</code></td>
      <td>Don't use aria2c even if available</td>
    </tr>
  </tbody>
</table>

**Advantages over SPARQL:**
- No timeouts (processes locally)
- Complete coverage (all notable people/orgs with English Wikipedia)
- Captures people via occupation (P106) even if position type is generic
- Extracts role dates from position qualifiers (P580/P582)
- Imports locations with hierarchical parent relationships (v0.9.4)

**Download location:** `~/.cache/corp-extractor/wikidata-latest-all.json.bz2`

**Note:** Use `-v` (verbose) to see detailed logs of skipped records during import:
```bash
corp-extractor db import-people --type executive -v
```

People records include `from_date` and `to_date` for role tenure. The same person can have multiple records with different role/org combinations (unique on `source_id + role + org`).

Organizations discovered during people import (employers, affiliated orgs) are automatically inserted into the organizations table if they don't already exist. This creates foreign key links via `known_for_org_id`.

#### Database Variants

<table>
  <thead>
    <tr>
      <th>File</th>
      <th>Description</th>
      <th>Use Case</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>entities-lite.db</code></td>
      <td>Core fields + embeddings only</td>
      <td>Default download, fast searches</td>
    </tr>
    <tr>
      <td><code>entities.db</code></td>
      <td>Full database with source metadata</td>
      <td>When you need complete record data</td>
    </tr>
    <tr>
      <td><code>*.db.gz</code></td>
      <td>Gzip compressed versions</td>
      <td>Faster downloads, auto-decompressed</td>
    </tr>
  </tbody>
</table>

#### Database Options

<table>
  <thead>
    <tr>
      <th>Option</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>--db PATH</code></td>
      <td>Database file path</td>
      <td>~/.cache/corp-extractor/entities.db</td>
    </tr>
    <tr>
      <td><code>--limit N</code></td>
      <td>Limit number of records</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--download</code></td>
      <td>Download source data automatically</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--full</code></td>
      <td>Download full version instead of lite</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--no-lite</code></td>
      <td>Skip creating lite version on upload</td>
      <td>—</td>
    </tr>
    <tr>
      <td><code>--no-compress</code></td>
      <td>Skip creating compressed versions</td>
      <td>—</td>
    </tr>
  </tbody>
</table>

See [COMPANY_DB.md](https://github.com/corp-o-rate/statement-extractor/blob/main/COMPANY_DB.md) for complete build and publish instructions.
