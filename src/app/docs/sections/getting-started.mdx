## Getting Started

<span id="getting-started" />

### Installation

<span id="installation" />

```bash
pip install corp-extractor
```

The GLiNER2 model (205M params) is downloaded automatically on first use.

**GPU support**: Install PyTorch with CUDA before installing corp-extractor. The library auto-detects GPU availability at runtime.

```bash
# Example for CUDA 12.1
pip install torch --index-url https://download.pytorch.org/whl/cu121
pip install corp-extractor
```

**Apple Silicon (M1/M2/M3)**: MPS acceleration is automatically detected. Just install normally:

```bash
pip install corp-extractor
```

### Quick Start

<span id="quick-start" />

Extract structured statements from text in 5 lines:

```python
from statement_extractor import extract_statements

text = "Apple Inc. acquired Beats Electronics for $3 billion in May 2014."
statements = extract_statements(text)

for stmt in statements:
    print(f"{stmt.subject.text} ({stmt.subject.type}) -> {stmt.predicate} -> {stmt.object.text}")
```

Output:

```
Apple Inc. (ORG) -> acquired -> Beats Electronics
Apple Inc. (ORG) -> paid -> $3 billion
Beats Electronics (ORG) -> acquisition price -> $3 billion
```

Each statement includes confidence scores and extraction method:

```python
for stmt in statements:
    print(f"{stmt.subject.text} -> {stmt.predicate} -> {stmt.object.text}")
    print(f"  method: {stmt.extraction_method}")  # hybrid, gliner, or model
    print(f"  confidence: {stmt.confidence_score:.2f}")
```

**v0.5.0 features**: Plugin-based pipeline architecture with entity qualification, labeling, and taxonomy classification. GLiNER2 entity recognition, entity-based scoring.

**v0.6.0 features**: Entity embedding database with ~100K+ SEC filers, ~3M GLEIF records, ~5M UK organizations for fast entity qualification.

**v0.7.0 features**: Document processing for files, URLs, and PDFs with automatic chunking, deduplication, and citation tracking.

**v0.8.0 features**: Merged qualification and canonicalization into single stage. EntityType classification for organizations (business, nonprofit, government, etc.).

**v0.9.0 features**: Person database with Wikidata import for notable people (executives, politicians, athletes, artists). PersonQualifier for canonical person identification with role/org context.

**v0.9.1 features**: Wikidata dump importer (`import-wikidata-dump`) for large imports without SPARQL timeouts. Uses aria2c for fast parallel downloads. Extracts people via occupation (P106) and position dates (P580/P582).

**v0.9.2 features**: Organization canonicalization links equivalent records across sources (GLEIF, SEC, Companies House, Wikidata). People canonicalization with priority-based deduplication. Expanded PersonType classification (executive, politician, government, military, legal, etc.).

**v0.9.3 features**: SEC Form 4 officers import (`import-sec-officers`) and Companies House officers import (`import-ch-officers`). People now sourced from Wikidata, SEC Edgar, and Companies House with cross-source canonicalization.

**v0.9.4 features**: Database v2 schema with normalized INTEGER foreign keys and enum lookup tables. Scalar (int8) embeddings for 75% storage reduction with ~92% recall. New locations import for countries/states/cities with hierarchy. Migration commands: `db migrate-v2`, `db backfill-scalar`. New search commands: `db search-roles`, `db search-locations`.

### Pipeline Quick Start (v0.5.0)

For full entity resolution with qualification, canonicalization, labeling, and taxonomy classification:

```python
from statement_extractor.pipeline import ExtractionPipeline

pipeline = ExtractionPipeline()
ctx = pipeline.process("Amazon CEO Andy Jassy announced plans to hire workers.")

# Access fully qualified names (e.g., "Andy Jassy (CEO, Amazon)")
for stmt in ctx.labeled_statements:
    print(f"{stmt.subject_fqn} --[{stmt.statement.predicate}]--> {stmt.object_fqn}")

    # Access labels (sentiment, etc.)
    for label in stmt.labels:
        print(f"  {label.label_type}: {label.label_value}")
```

CLI usage:

```bash
# Full pipeline
corp-extractor pipeline "Amazon CEO Andy Jassy announced..."

# Run specific stages only
corp-extractor pipeline -f article.txt --stages 1-3

# Process documents and URLs (v0.7.0)
corp-extractor document process article.txt
corp-extractor document process https://example.com/article
corp-extractor document process report.pdf --use-ocr
```

### Using Predicate Taxonomies

Normalize extracted predicates to canonical forms using embedding similarity:

```python
from statement_extractor import extract_statements, PredicateTaxonomy, ExtractionOptions

# Define your domain's canonical predicates
taxonomy = PredicateTaxonomy(predicates=[
    "acquired", "founded", "works_for", "headquartered_in",
    "invested_in", "partnered_with", "announced"
])

options = ExtractionOptions(predicate_taxonomy=taxonomy)

text = "Google bought YouTube for $1.65 billion in 2006."
result = extract_statements(text, options)

for stmt in result:
    print(f"{stmt.predicate} -> {stmt.canonical_predicate}")
    # Output: bought -> acquired
```

This maps synonyms like "bought", "purchased", "acquired" to a single canonical form, making downstream analysis easier.

### Requirements

<span id="requirements" />

<table>
  <thead>
    <tr>
      <th>Dependency</th>
      <th>Version</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Python</td>
      <td>3.10+</td>
      <td>Required</td>
    </tr>
    <tr>
      <td>PyTorch</td>
      <td>2.0+</td>
      <td>Required</td>
    </tr>
    <tr>
      <td>transformers</td>
      <td>5.0+</td>
      <td>Required for T5-Gemma2 support</td>
    </tr>
    <tr>
      <td>Pydantic</td>
      <td>2.0+</td>
      <td>Required</td>
    </tr>
    <tr>
      <td>sentence-transformers</td>
      <td>2.2+</td>
      <td>Required, for embedding features</td>
    </tr>
    <tr>
      <td>GLiNER2</td>
      <td>latest</td>
      <td>Required, for entity recognition and relation extraction (model auto-downloads)</td>
    </tr>
  </tbody>
</table>

**Hardware requirements**:

- **NVIDIA GPU**: RTX 4090+ recommended for production. Uses bfloat16 precision for efficiency.
- **Apple Silicon**: M1/M2/M3 with 16GB+ RAM. MPS acceleration auto-detected.
- **CPU**: Functional but slower. Use for development or low-volume processing.
- **Disk**: ~100GB for all models and entity database (10M+ organizations, 40M+ people).

The library runs entirely locally with no external API dependencies. Models use bfloat16 on CUDA and float32 on MPS/CPU.
