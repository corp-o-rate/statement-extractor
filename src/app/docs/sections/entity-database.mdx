## Entity Database

<span id="entity-database" />

The entity database provides fast lookup and qualification of organizations and people using vector similarity search. It stores records from authoritative sources with 768-dimensional embeddings for semantic matching.

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">NEW in v0.9.0</span>

---

### Quick Start

<span id="entity-db-quickstart" />

```bash
# Download the pre-built database
corp-extractor db download

# Check what's in it
corp-extractor db status

# Search for organizations
corp-extractor db search "Microsoft"

# Search for people
corp-extractor db search-people "Tim Cook"
```

The database is automatically used by the pipeline's qualification stage (Stage 3) to resolve entity names to canonical identifiers.

---

### Getting the Database

<span id="getting-database" />

#### Download Pre-built Database

The fastest way to get started is downloading from HuggingFace:

```bash
# Download lite version (default, smaller, faster)
corp-extractor db download

# Download full version (includes complete source metadata)
corp-extractor db download --full
```

**Database variants:**

<table>
  <thead>
    <tr>
      <th>File</th>
      <th>Size</th>
      <th>Contents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>entities-lite.db</code></td>
      <td>~50GB</td>
      <td>Core fields + embeddings only</td>
    </tr>
    <tr>
      <td><code>entities.db</code></td>
      <td>~74GB</td>
      <td>Full records with source metadata</td>
    </tr>
  </tbody>
</table>

**Storage location:** `~/.cache/corp-extractor/entities.db`

**HuggingFace repo:** [Corp-o-Rate-Community/entity-references](https://huggingface.co/datasets/Corp-o-Rate-Community/entity-references)

#### Automatic Download

If you use the pipeline without downloading first, the database downloads automatically:

```python
from statement_extractor.pipeline import ExtractionPipeline

pipeline = ExtractionPipeline()
ctx = pipeline.process("Microsoft CEO Satya Nadella announced...")
# Database downloaded automatically if not present
```

---

### Database Schema

<span id="database-schema" />

The database uses SQLite with the [sqlite-vec](https://github.com/asg017/sqlite-vec) extension for vector similarity search.

#### Organizations Table

```sql
CREATE TABLE organizations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    name_normalized TEXT NOT NULL,
    source TEXT NOT NULL,        -- 'gleif', 'sec_edgar', 'companies_house', 'wikipedia'
    source_id TEXT NOT NULL,     -- LEI, CIK, UK Company Number, or QID
    region TEXT NOT NULL DEFAULT '',
    entity_type TEXT NOT NULL DEFAULT 'unknown',
    from_date TEXT,              -- ISO YYYY-MM-DD
    to_date TEXT,                -- ISO YYYY-MM-DD
    record TEXT NOT NULL,        -- JSON (empty in lite version)
    UNIQUE(source, source_id)
);

CREATE VIRTUAL TABLE organization_embeddings USING vec0(
    org_id INTEGER PRIMARY KEY,
    embedding float[768]
);
```

#### People Table

```sql
CREATE TABLE people (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name TEXT NOT NULL,
    name_normalized TEXT NOT NULL,
    source TEXT NOT NULL,        -- 'wikidata'
    source_id TEXT NOT NULL,     -- Wikidata QID
    country TEXT NOT NULL DEFAULT '',
    person_type TEXT NOT NULL DEFAULT 'unknown',
    known_for_role TEXT DEFAULT '',
    known_for_org TEXT DEFAULT '',
    known_for_org_id INTEGER,    -- FK to organizations table
    from_date TEXT,              -- Role start date (ISO)
    to_date TEXT,                -- Role end date (ISO)
    birth_date TEXT,             -- ISO YYYY-MM-DD
    death_date TEXT,             -- ISO YYYY-MM-DD
    record TEXT NOT NULL,
    UNIQUE(source, source_id, known_for_role, known_for_org)
);

CREATE VIRTUAL TABLE person_embeddings USING vec0(
    person_id INTEGER PRIMARY KEY,
    embedding float[768]
);
```

---

### Entity Types

<span id="entity-db-types" />

#### Organization EntityTypes

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Business</td>
      <td><code>business</code>, <code>fund</code>, <code>branch</code></td>
    </tr>
    <tr>
      <td>Non-profit</td>
      <td><code>nonprofit</code>, <code>ngo</code>, <code>foundation</code>, <code>trade_union</code></td>
    </tr>
    <tr>
      <td>Government</td>
      <td><code>government</code>, <code>international_org</code>, <code>political_party</code></td>
    </tr>
    <tr>
      <td>Education</td>
      <td><code>educational</code>, <code>research</code></td>
    </tr>
    <tr>
      <td>Other</td>
      <td><code>healthcare</code>, <code>media</code>, <code>sports</code>, <code>religious</code>, <code>unknown</code></td>
    </tr>
  </tbody>
</table>

#### Person PersonTypes

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>executive</code></td>
      <td>C-suite, board members</td>
      <td>Tim Cook, Satya Nadella</td>
    </tr>
    <tr>
      <td><code>politician</code></td>
      <td>Elected officials</td>
      <td>Presidents, MPs, mayors</td>
    </tr>
    <tr>
      <td><code>government</code></td>
      <td>Civil servants, diplomats</td>
      <td>Agency heads, ambassadors</td>
    </tr>
    <tr>
      <td><code>military</code></td>
      <td>Armed forces personnel</td>
      <td>Generals, admirals</td>
    </tr>
    <tr>
      <td><code>legal</code></td>
      <td>Judges, lawyers</td>
      <td>Supreme Court justices</td>
    </tr>
    <tr>
      <td><code>professional</code></td>
      <td>Known for profession</td>
      <td>Famous surgeons, architects</td>
    </tr>
    <tr>
      <td><code>academic</code></td>
      <td>Professors, researchers</td>
      <td>Neil deGrasse Tyson</td>
    </tr>
    <tr>
      <td><code>scientist</code></td>
      <td>Scientists, inventors</td>
      <td>Research scientists</td>
    </tr>
    <tr>
      <td><code>athlete</code></td>
      <td>Sports figures</td>
      <td>LeBron James</td>
    </tr>
    <tr>
      <td><code>artist</code></td>
      <td>Traditional creatives</td>
      <td>Musicians, actors, painters</td>
    </tr>
    <tr>
      <td><code>media</code></td>
      <td>Internet personalities</td>
      <td>YouTubers, influencers</td>
    </tr>
    <tr>
      <td><code>journalist</code></td>
      <td>Reporters, presenters</td>
      <td>Anderson Cooper</td>
    </tr>
    <tr>
      <td><code>entrepreneur</code></td>
      <td>Founders, business owners</td>
      <td>Mark Zuckerberg</td>
    </tr>
    <tr>
      <td><code>activist</code></td>
      <td>Advocates, campaigners</td>
      <td>Greta Thunberg</td>
    </tr>
  </tbody>
</table>

---

### Data Sources

<span id="data-sources" />

#### Organizations

<table>
  <thead>
    <tr>
      <th>Source</th>
      <th>Records</th>
      <th>Identifier</th>
      <th>Coverage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GLEIF</td>
      <td>~3.2M</td>
      <td>LEI (Legal Entity Identifier)</td>
      <td>Global legal entities</td>
    </tr>
    <tr>
      <td>SEC Edgar</td>
      <td>~100K+</td>
      <td>CIK (Central Index Key)</td>
      <td>US public companies</td>
    </tr>
    <tr>
      <td>Companies House</td>
      <td>~5M</td>
      <td>UK Company Number</td>
      <td>UK registered companies</td>
    </tr>
    <tr>
      <td>Wikidata</td>
      <td>Variable</td>
      <td>QID</td>
      <td>Notable organizations</td>
    </tr>
  </tbody>
</table>

#### People

<table>
  <thead>
    <tr>
      <th>Source</th>
      <th>Records</th>
      <th>Identifier</th>
      <th>Coverage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Wikidata</td>
      <td>Variable</td>
      <td>QID</td>
      <td>Notable people worldwide</td>
    </tr>
  </tbody>
</table>

---

### Python API

<span id="entity-db-python" />

#### Search Organizations

```python
from statement_extractor.database import OrganizationDatabase

db = OrganizationDatabase()

# Search by name (hybrid: text + embedding)
matches = db.search_by_name("Microsoft Corporation", top_k=5)
for match in matches:
    print(f"{match.company.name} ({match.company.source}:{match.company.source_id})")
    print(f"  Similarity: {match.similarity_score:.3f}")
    print(f"  Type: {match.company.entity_type}")

# Search by embedding
from statement_extractor.database import CompanyEmbedder

embedder = CompanyEmbedder()
embedding = embedder.embed("Microsoft")
matches = db.search(embedding, top_k=10, min_similarity=0.7)
```

#### Search People

```python
from statement_extractor.database import PersonDatabase

db = PersonDatabase()

# Search by name
matches = db.search_by_name("Tim Cook", top_k=5)
for match in matches:
    print(f"{match.person.name} - {match.person.known_for_role} at {match.person.known_for_org}")
    print(f"  Wikidata: {match.person.source_id}")
    print(f"  Type: {match.person.person_type}")
```

#### Use in Pipeline

The database is automatically used by qualification plugins:

```python
from statement_extractor.pipeline import ExtractionPipeline

pipeline = ExtractionPipeline()
ctx = pipeline.process("Microsoft CEO Satya Nadella announced new AI features.")

for stmt in ctx.labeled_statements:
    print(f"{stmt.subject_fqn} --[{stmt.statement.predicate}]--> {stmt.object_fqn}")
    # e.g., "Satya Nadella (CEO, Microsoft) --[announced]--> new AI features"
```

#### Add Custom Records

```python
from statement_extractor.database import OrganizationDatabase, CompanyRecord, EntityType

db = OrganizationDatabase()

record = CompanyRecord(
    name="My Company Inc",
    source="custom",
    source_id="CUSTOM001",
    region="US",
    entity_type=EntityType.business,
    record={"custom_field": "value"},
)
db.add_record(record)
```

---

### Building Your Own Database

<span id="building-database" />

#### Import Organizations

```bash
# GLEIF - Global LEI data (~3M records)
corp-extractor db import-gleif --download
corp-extractor db import-gleif /path/to/lei-data.json --limit 50000

# SEC Edgar - US public companies (~100K+ filers)
corp-extractor db import-sec --download

# UK Companies House (~5M records)
corp-extractor db import-companies-house --download

# Wikidata organizations via SPARQL
corp-extractor db import-wikidata --limit 50000
```

#### Import People

```bash
# Import by person type
corp-extractor db import-people --type executive --limit 5000
corp-extractor db import-people --type politician --limit 5000
corp-extractor db import-people --type athlete --limit 5000

# Import all person types
corp-extractor db import-people --all --limit 50000

# Skip existing records (faster for incremental updates)
corp-extractor db import-people --type executive --skip-existing

# Fetch role start/end dates (slower, queries per person)
corp-extractor db import-people --type executive --enrich-dates
```

#### Wikidata Dump Import

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.1</span>

For large imports without SPARQL query timeouts:

```bash
# Download and import from Wikidata dump (~100GB compressed)
corp-extractor db import-wikidata-dump --download --limit 100000

# Import from local dump file
corp-extractor db import-wikidata-dump --dump /path/to/latest-all.json.bz2

# Import only people (no organizations)
corp-extractor db import-wikidata-dump --dump dump.bz2 --people --no-orgs

# Resume interrupted import
corp-extractor db import-wikidata-dump --dump dump.bz2 --resume

# Skip records already in database
corp-extractor db import-wikidata-dump --dump dump.bz2 --skip-updates
```

**Fast download with aria2c:** Install `aria2c` for 10-20x faster downloads:
```bash
brew install aria2   # macOS
apt install aria2    # Ubuntu/Debian
```

#### Full Build Process

```bash
# 1. Import from all sources
corp-extractor db import-gleif --download
corp-extractor db import-sec --download
corp-extractor db import-companies-house --download
corp-extractor db import-wikidata --limit 100000
corp-extractor db import-wikidata-dump --download --people --no-orgs --limit 100000

# 2. Link equivalent records
corp-extractor db canonicalize

# 3. Check status
corp-extractor db status

# 4. Upload to HuggingFace
export HF_TOKEN="hf_..."
corp-extractor db upload
```

---

### Canonicalization

<span id="canonicalization" />

Link equivalent records across sources:

```bash
corp-extractor db canonicalize
```

#### Organizations

Matches organizations by:
- **Global identifiers**: LEI, CIK, ticker (no region check needed)
- **Normalized name + region**: Handles suffix variations (Ltd → Limited, Corp → Corporation)

**Source priority**: gleif > sec_edgar > companies_house > wikipedia

#### People

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.3</span>

Matches people by:
- **Normalized name + same organization**: Uses org canonical group to link people across sources
- **Normalized name + overlapping date ranges**: Links records with matching tenure periods

**Source priority**: wikidata > sec_edgar > companies_house

Canonicalization enables prominence-based search re-ranking that boosts entities with records from multiple authoritative sources.

---

### Data Models

<span id="data-models-db" />

#### CompanyRecord

```python
class CompanyRecord(BaseModel):
    name: str                    # Organization name
    source: str                  # 'gleif', 'sec_edgar', 'companies_house', 'wikipedia'
    source_id: str               # LEI, CIK, UK Company Number, or QID
    region: str                  # Country/region code
    entity_type: EntityType      # Classification
    from_date: Optional[str]     # ISO YYYY-MM-DD
    to_date: Optional[str]       # ISO YYYY-MM-DD
    record: dict[str, Any]       # Full source record (empty in lite)

    @property
    def canonical_id(self) -> str:
        return f"{self.source}:{self.source_id}"
```

#### PersonRecord

```python
class PersonRecord(BaseModel):
    name: str                    # Display name
    source: str                  # 'wikidata'
    source_id: str               # Wikidata QID
    country: str                 # Country code
    person_type: PersonType      # Classification
    known_for_role: str          # Primary role
    known_for_org: str           # Primary organization name
    known_for_org_id: Optional[int]  # FK to organizations
    from_date: Optional[str]     # Role start (ISO)
    to_date: Optional[str]       # Role end (ISO)
    birth_date: Optional[str]    # Birth date (ISO)
    death_date: Optional[str]    # Death date (ISO)
    record: dict[str, Any]       # Full source record

    @property
    def is_historic(self) -> bool:
        return self.death_date is not None
```

#### Match Results

```python
class CompanyMatch(BaseModel):
    company: CompanyRecord
    similarity_score: float      # 0.0 to 1.0

class PersonMatch(BaseModel):
    person: PersonRecord
    similarity_score: float      # 0.0 to 1.0
    llm_confirmed: bool          # Whether LLM validated match
```

---

### Embedding Model

<span id="embedding-model" />

Embeddings are generated using [google/embeddinggemma-300m](https://huggingface.co/google/embeddinggemma-300m):

- **Parameters**: 300M (lightweight)
- **Dimensions**: 768
- **Optimized for**: CPU inference
- **Auto-download**: Model downloads automatically on first use

```python
from statement_extractor.database import CompanyEmbedder

embedder = CompanyEmbedder()
embedding = embedder.embed("Apple Inc")  # Returns 768-dim numpy array
```

---

### Troubleshooting

<span id="troubleshooting-db" />

**Database not found:**
```
Error: Database not found at ~/.cache/corp-extractor/entities.db
```
Run `corp-extractor db download` to fetch the pre-built database.

**sqlite-vec extension error:**
```
Error: no such module: vec0
```
The sqlite-vec extension should install automatically. If not: `pip install sqlite-vec`

**Memory issues with large dumps:**
```bash
# Import in smaller batches
corp-extractor db import-wikidata-dump --dump dump.bz2 --limit 10000 --skip-updates
# Then resume for more
corp-extractor db import-wikidata-dump --dump dump.bz2 --limit 10000 --skip-updates --resume
```

**Resume interrupted import:**
```bash
corp-extractor db import-wikidata-dump --dump dump.bz2 --resume
```
Progress is saved to `~/.cache/corp-extractor/wikidata-dump-progress.json`.
