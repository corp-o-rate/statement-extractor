## Entity Database

<a id="entity-database-main" />

The entity database provides fast lookup and qualification of organizations, people, roles, and locations using vector similarity search. It stores records from authoritative sources with 768-dimensional embeddings for semantic matching.

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">UPDATED in v0.9.4</span>

---

### Quick Start

<span id="entity-db-quickstart" />

```bash
# Download the pre-built database
corp-extractor db download

# Check what's in it
corp-extractor db status

# Search for organizations
corp-extractor db search "Microsoft"

# Search for people
corp-extractor db search-people "Tim Cook"

# Search for roles (v0.9.4)
corp-extractor db search-roles "CEO"

# Search for locations (v0.9.4)
corp-extractor db search-locations "California"
```

The database is automatically used by the pipeline's qualification stage (Stage 3) to resolve entity names to canonical identifiers.

---

### Getting the Database

<span id="getting-database" />

#### Download Pre-built Database

The fastest way to get started is downloading from HuggingFace:

```bash
# Download lite version (default, smaller, faster)
corp-extractor db download

# Download full version (includes complete source metadata)
corp-extractor db download --full
```

**Database variants:**

<table>
  <thead>
    <tr>
      <th>File</th>
      <th>Size</th>
      <th>Contents</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>entities-lite.db</code></td>
      <td>30.1 GB</td>
      <td>Core fields + int8 embeddings only</td>
    </tr>
    <tr>
      <td><code>entities.db</code></td>
      <td>32.2 GB</td>
      <td>Full records with source metadata</td>
    </tr>
  </tbody>
</table>

**Storage location:** `~/.cache/corp-extractor/entities-v2.db` (v0.9.4+)

**HuggingFace repo:** [Corp-o-Rate-Community/entity-references](https://huggingface.co/datasets/Corp-o-Rate-Community/entity-references)

#### Automatic Download

If you use the pipeline without downloading first, the database downloads automatically:

```python
from statement_extractor.pipeline import ExtractionPipeline

pipeline = ExtractionPipeline()
ctx = pipeline.process("Microsoft CEO Satya Nadella announced...")
# Database downloaded automatically if not present
```

---

### Database Schema

<span id="database-schema" />

The database uses SQLite with the [sqlite-vec](https://github.com/asg017/sqlite-vec) extension for vector similarity search.

#### Schema v2 (Normalized)

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.4</span>

The v2 schema uses INTEGER foreign keys to enum lookup tables instead of TEXT columns:

```sql
-- Enum tables: source_types, people_types, organization_types, location_types
-- Organization: source_id (FK), entity_type_id (FK), region_id (FK to locations)
-- People: source_id (FK), person_type_id (FK), country_id (FK), known_for_role_id (FK)
-- Roles: qid, name, source_id (FK), canon_id
-- Locations: qid, name, source_id (FK), location_type_id (FK), parent_ids (hierarchy)
```

#### Organizations Table

```sql
CREATE TABLE organizations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    qid INTEGER,                 -- Wikidata QID as integer (v0.9.4)
    name TEXT NOT NULL,
    name_normalized TEXT NOT NULL,
    source_id INTEGER NOT NULL,  -- FK to source_types(id)
    source_identifier TEXT NOT NULL,  -- LEI, CIK, Company Number
    region_id INTEGER,           -- FK to locations(id) (v0.9.4)
    entity_type_id INTEGER NOT NULL,  -- FK to organization_types(id)
    from_date TEXT,              -- ISO YYYY-MM-DD
    to_date TEXT,                -- ISO YYYY-MM-DD
    record TEXT NOT NULL,        -- JSON (empty in lite version)
    UNIQUE(source_identifier, source_id)
);

-- Both float32 and int8 embeddings supported (v0.9.4)
CREATE VIRTUAL TABLE organization_embeddings USING vec0(
    org_id INTEGER PRIMARY KEY, embedding float[768]
);
CREATE VIRTUAL TABLE organization_embeddings_scalar USING vec0(
    org_id INTEGER PRIMARY KEY, embedding int8[768]
);
```

#### People Table

```sql
CREATE TABLE people (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    qid INTEGER,                 -- Wikidata QID as integer (v0.9.4)
    name TEXT NOT NULL,
    name_normalized TEXT NOT NULL,
    source_id INTEGER NOT NULL,  -- FK to source_types(id)
    source_identifier TEXT NOT NULL,  -- QID, Owner CIK, Person number
    country_id INTEGER,          -- FK to locations(id) (v0.9.4)
    person_type_id INTEGER NOT NULL,  -- FK to people_types(id)
    known_for_role_id INTEGER,   -- FK to roles(id) (v0.9.4)
    known_for_org TEXT DEFAULT '',
    known_for_org_id INTEGER,    -- FK to organizations(id)
    from_date TEXT,              -- Role start date (ISO)
    to_date TEXT,                -- Role end date (ISO)
    birth_date TEXT,             -- ISO YYYY-MM-DD
    death_date TEXT,             -- ISO YYYY-MM-DD
    record TEXT NOT NULL,
    UNIQUE(source_identifier, source_id, known_for_role_id, known_for_org_id)
);

CREATE VIRTUAL TABLE person_embeddings USING vec0(
    person_id INTEGER PRIMARY KEY, embedding float[768]
);
CREATE VIRTUAL TABLE person_embeddings_scalar USING vec0(
    person_id INTEGER PRIMARY KEY, embedding int8[768]
);
```

#### New Tables (v0.9.4)

```sql
-- Roles table for job titles
CREATE TABLE roles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    qid INTEGER,                 -- Wikidata QID (e.g., 484876 for CEO)
    name TEXT NOT NULL,          -- "Chief Executive Officer"
    name_normalized TEXT NOT NULL,
    source_id INTEGER NOT NULL,  -- FK to source_types(id)
    canon_id INTEGER DEFAULT NULL,
    UNIQUE(name_normalized, source_id)
);

-- Locations table for geopolitical entities
CREATE TABLE locations (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    qid INTEGER,                 -- Wikidata QID (e.g., 30 for USA)
    name TEXT NOT NULL,          -- "United States", "California"
    name_normalized TEXT NOT NULL,
    source_id INTEGER NOT NULL,  -- FK to source_types(id)
    source_identifier TEXT,      -- "US", "CA"
    parent_ids TEXT,             -- JSON array of parent location IDs
    location_type_id INTEGER NOT NULL,  -- FK to location_types(id)
    UNIQUE(source_identifier, source_id)
);
```

---

### Entity Types

<span id="entity-db-types" />

#### Organization EntityTypes

<table>
  <thead>
    <tr>
      <th>Category</th>
      <th>Types</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Business</td>
      <td><code>business</code>, <code>fund</code>, <code>branch</code></td>
    </tr>
    <tr>
      <td>Non-profit</td>
      <td><code>nonprofit</code>, <code>ngo</code>, <code>foundation</code>, <code>trade_union</code></td>
    </tr>
    <tr>
      <td>Government</td>
      <td><code>government</code>, <code>international_org</code>, <code>political_party</code></td>
    </tr>
    <tr>
      <td>Education</td>
      <td><code>educational</code>, <code>research</code></td>
    </tr>
    <tr>
      <td>Other</td>
      <td><code>healthcare</code>, <code>media</code>, <code>sports</code>, <code>religious</code>, <code>unknown</code></td>
    </tr>
  </tbody>
</table>

#### Person PersonTypes

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>executive</code></td>
      <td>C-suite, board members</td>
      <td>Tim Cook, Satya Nadella</td>
    </tr>
    <tr>
      <td><code>politician</code></td>
      <td>Elected officials</td>
      <td>Presidents, MPs, mayors</td>
    </tr>
    <tr>
      <td><code>government</code></td>
      <td>Civil servants, diplomats</td>
      <td>Agency heads, ambassadors</td>
    </tr>
    <tr>
      <td><code>military</code></td>
      <td>Armed forces personnel</td>
      <td>Generals, admirals</td>
    </tr>
    <tr>
      <td><code>legal</code></td>
      <td>Judges, lawyers</td>
      <td>Supreme Court justices</td>
    </tr>
    <tr>
      <td><code>professional</code></td>
      <td>Known for profession</td>
      <td>Famous surgeons, architects</td>
    </tr>
    <tr>
      <td><code>academic</code></td>
      <td>Professors, researchers</td>
      <td>Neil deGrasse Tyson</td>
    </tr>
    <tr>
      <td><code>scientist</code></td>
      <td>Scientists, inventors</td>
      <td>Research scientists</td>
    </tr>
    <tr>
      <td><code>athlete</code></td>
      <td>Sports figures</td>
      <td>LeBron James</td>
    </tr>
    <tr>
      <td><code>artist</code></td>
      <td>Traditional creatives</td>
      <td>Musicians, actors, painters</td>
    </tr>
    <tr>
      <td><code>media</code></td>
      <td>Internet personalities</td>
      <td>YouTubers, influencers</td>
    </tr>
    <tr>
      <td><code>journalist</code></td>
      <td>Reporters, presenters</td>
      <td>Anderson Cooper</td>
    </tr>
    <tr>
      <td><code>entrepreneur</code></td>
      <td>Founders, business owners</td>
      <td>Mark Zuckerberg</td>
    </tr>
    <tr>
      <td><code>activist</code></td>
      <td>Advocates, campaigners</td>
      <td>Greta Thunberg</td>
    </tr>
  </tbody>
</table>

#### Simplified Location Types

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.4</span>

<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
      <th>Examples</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>continent</code></td>
      <td>Continents</td>
      <td>Europe, Asia, Africa</td>
    </tr>
    <tr>
      <td><code>country</code></td>
      <td>Sovereign states</td>
      <td>United States, Germany, Japan</td>
    </tr>
    <tr>
      <td><code>subdivision</code></td>
      <td>States, provinces, regions</td>
      <td>California, Bavaria, Ontario</td>
    </tr>
    <tr>
      <td><code>city</code></td>
      <td>Cities, towns, municipalities</td>
      <td>New York, Paris, Tokyo</td>
    </tr>
    <tr>
      <td><code>district</code></td>
      <td>Districts, boroughs, neighborhoods</td>
      <td>Manhattan, Westminster</td>
    </tr>
    <tr>
      <td><code>historic</code></td>
      <td>Former countries, historic territories</td>
      <td>Soviet Union, Prussia</td>
    </tr>
  </tbody>
</table>

---

### Data Sources

<span id="data-sources" />

#### Organizations

<table>
  <thead>
    <tr>
      <th>Source</th>
      <th>Records</th>
      <th>Identifier</th>
      <th>Coverage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Companies House</td>
      <td>5.5M</td>
      <td>UK Company Number</td>
      <td>UK registered companies</td>
    </tr>
    <tr>
      <td>GLEIF</td>
      <td>2.6M</td>
      <td>LEI (Legal Entity Identifier)</td>
      <td>Global legal entities</td>
    </tr>
    <tr>
      <td>Wikidata</td>
      <td>1.5M</td>
      <td>QID</td>
      <td>Notable organizations</td>
    </tr>
    <tr>
      <td>SEC Edgar</td>
      <td>73K</td>
      <td>CIK (Central Index Key)</td>
      <td>US public companies</td>
    </tr>
  </tbody>
</table>

**Total: 9.6M+ organizations**

#### People

<table>
  <thead>
    <tr>
      <th>Source</th>
      <th>Records</th>
      <th>Identifier</th>
      <th>Coverage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Companies House</td>
      <td>27.5M</td>
      <td>Person number</td>
      <td>UK company officers</td>
    </tr>
    <tr>
      <td>Wikidata</td>
      <td>13.4M</td>
      <td>QID</td>
      <td>Notable people worldwide</td>
    </tr>
  </tbody>
</table>

**Total: 41M+ people**

#### Other Tables

<table>
  <thead>
    <tr>
      <th>Table</th>
      <th>Records</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Roles</td>
      <td>94K</td>
      <td>Job titles with Wikidata QIDs</td>
    </tr>
    <tr>
      <td>Locations</td>
      <td>25K</td>
      <td>Countries, states, cities with hierarchy</td>
    </tr>
  </tbody>
</table>

---

### Python API

<span id="entity-db-python" />

#### Search Organizations

```python
from statement_extractor.database import OrganizationDatabase

db = OrganizationDatabase()

# Search by name (hybrid: text + embedding)
matches = db.search_by_name("Microsoft Corporation", top_k=5)
for match in matches:
    print(f"{match.company.name} ({match.company.source}:{match.company.source_id})")
    print(f"  Similarity: {match.similarity_score:.3f}")
    print(f"  Type: {match.company.entity_type}")

# Search by embedding
from statement_extractor.database import CompanyEmbedder

embedder = CompanyEmbedder()
embedding = embedder.embed("Microsoft")
matches = db.search(embedding, top_k=10, min_similarity=0.7)
```

#### Search People

```python
from statement_extractor.database import PersonDatabase

db = PersonDatabase()

# Search by name
matches = db.search_by_name("Tim Cook", top_k=5)
for match in matches:
    print(f"{match.person.name} - {match.person.known_for_role} at {match.person.known_for_org}")
    print(f"  Wikidata: {match.person.source_id}")
    print(f"  Type: {match.person.person_type}")
```

#### Use in Pipeline

The database is automatically used by qualification plugins:

```python
from statement_extractor.pipeline import ExtractionPipeline

pipeline = ExtractionPipeline()
ctx = pipeline.process("Microsoft CEO Satya Nadella announced new AI features.")

for stmt in ctx.labeled_statements:
    print(f"{stmt.subject_fqn} --[{stmt.statement.predicate}]--> {stmt.object_fqn}")
    # e.g., "Satya Nadella (CEO, Microsoft) --[announced]--> new AI features"
```

#### Add Custom Records

```python
from statement_extractor.database import OrganizationDatabase, CompanyRecord, EntityType

db = OrganizationDatabase()

record = CompanyRecord(
    name="My Company Inc",
    source="custom",
    source_id="CUSTOM001",
    region="US",
    entity_type=EntityType.business,
    record={"custom_field": "value"},
)
db.add_record(record)
```

---

### Building Your Own Database

<span id="building-database" />

#### Import Organizations

```bash
# Companies House - UK companies (5.5M records)
corp-extractor db import-companies-house --download

# GLEIF - Global LEI data (2.6M records)
corp-extractor db import-gleif --download
corp-extractor db import-gleif /path/to/lei-data.json --limit 50000

# SEC Edgar - US public companies (73K filers)
corp-extractor db import-sec --download

# Wikidata organizations via SPARQL (1.5M records)
corp-extractor db import-wikidata --limit 50000
```

#### Import People

```bash
# Import by person type
corp-extractor db import-people --type executive --limit 5000
corp-extractor db import-people --type politician --limit 5000
corp-extractor db import-people --type athlete --limit 5000

# Import all person types
corp-extractor db import-people --all --limit 50000

# Skip existing records (faster for incremental updates)
corp-extractor db import-people --type executive --skip-existing

# Fetch role start/end dates (slower, queries per person)
corp-extractor db import-people --type executive --enrich-dates
```

#### Wikidata Dump Import

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.4</span>

For large imports without SPARQL query timeouts:

```bash
# Download and import from Wikidata dump (~100GB compressed)
corp-extractor db import-wikidata-dump --download --limit 100000

# Import from local dump file
corp-extractor db import-wikidata-dump --dump /path/to/latest-all.json.bz2

# Import only people (no organizations)
corp-extractor db import-wikidata-dump --dump dump.bz2 --people --no-orgs

# Import only locations (countries, states, cities) - v0.9.4
corp-extractor db import-wikidata-dump --dump dump.bz2 --locations --no-people --no-orgs

# Resume interrupted import
corp-extractor db import-wikidata-dump --dump dump.bz2 --resume

# Skip records already in database
corp-extractor db import-wikidata-dump --dump dump.bz2 --skip-updates
```

**Fast download with aria2c:** Install `aria2c` for 10-20x faster downloads:
```bash
brew install aria2   # macOS
apt install aria2    # Ubuntu/Debian
```

#### Full Build Process

```bash
# 1. Import from all sources
corp-extractor db import-gleif --download
corp-extractor db import-sec --download
corp-extractor db import-companies-house --download
corp-extractor db import-wikidata --limit 100000
corp-extractor db import-wikidata-dump --download --people --no-orgs --limit 100000

# 2. Link equivalent records
corp-extractor db canonicalize

# 3. Generate scalar embeddings (75% storage reduction)
corp-extractor db backfill-scalar

# 4. Check status
corp-extractor db status

# 5. Upload to HuggingFace
export HF_TOKEN="hf_..."
corp-extractor db upload
```

#### Migrate to v2 Schema

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.4</span>

To migrate an existing v1 database to the normalized v2 schema:

```bash
# Create new v2 database (preserves original)
corp-extractor db migrate-v2 entities.db entities-v2.db

# Resume interrupted migration
corp-extractor db migrate-v2 entities.db entities-v2.db --resume
```

The v2 schema provides:
- INTEGER FK columns instead of TEXT enums (better performance)
- New enum lookup tables for type filtering
- New roles and locations tables
- QIDs as integers (Q prefix stripped)
- Human-readable views with JOINs

---

### Canonicalization

<span id="canonicalization" />

Link equivalent records across sources:

```bash
corp-extractor db canonicalize
```

#### Organizations

Matches organizations by:
- **Global identifiers**: LEI, CIK, ticker (no region check needed)
- **Normalized name + region**: Handles suffix variations (Ltd → Limited, Corp → Corporation)

**Source priority**: gleif > sec_edgar > companies_house > wikipedia

#### People

<span className="text-xs font-medium bg-green-100 dark:bg-green-900 text-green-800 dark:text-green-200 px-2 py-0.5 rounded">v0.9.3</span>

Matches people by:
- **Normalized name + same organization**: Uses org canonical group to link people across sources
- **Normalized name + overlapping date ranges**: Links records with matching tenure periods

**Source priority**: wikidata > sec_edgar > companies_house

Canonicalization enables prominence-based search re-ranking that boosts entities with records from multiple authoritative sources.

---

### Data Models

<span id="data-models-db" />

#### CompanyRecord

```python
class CompanyRecord(BaseModel):
    name: str                    # Organization name
    source: str                  # 'gleif', 'sec_edgar', 'companies_house', 'wikipedia'
    source_id: str               # LEI, CIK, UK Company Number, or QID
    region: str                  # Country/region code
    entity_type: EntityType      # Classification
    from_date: Optional[str]     # ISO YYYY-MM-DD
    to_date: Optional[str]       # ISO YYYY-MM-DD
    record: dict[str, Any]       # Full source record (empty in lite)

    @property
    def canonical_id(self) -> str:
        return f"{self.source}:{self.source_id}"
```

#### PersonRecord

```python
class PersonRecord(BaseModel):
    name: str                    # Display name
    source: str                  # 'wikidata'
    source_id: str               # Wikidata QID
    country: str                 # Country code
    person_type: PersonType      # Classification
    known_for_role: str          # Primary role
    known_for_org: str           # Primary organization name
    known_for_org_id: Optional[int]  # FK to organizations
    from_date: Optional[str]     # Role start (ISO)
    to_date: Optional[str]       # Role end (ISO)
    birth_date: Optional[str]    # Birth date (ISO)
    death_date: Optional[str]    # Death date (ISO)
    record: dict[str, Any]       # Full source record

    @property
    def is_historic(self) -> bool:
        return self.death_date is not None
```

#### Match Results

```python
class CompanyMatch(BaseModel):
    company: CompanyRecord
    similarity_score: float      # 0.0 to 1.0

class PersonMatch(BaseModel):
    person: PersonRecord
    similarity_score: float      # 0.0 to 1.0
    llm_confirmed: bool          # Whether LLM validated match
```

---

### Embedding Model

<span id="embedding-model" />

Embeddings are generated using [google/embeddinggemma-300m](https://huggingface.co/google/embeddinggemma-300m):

- **Parameters**: 300M (lightweight)
- **Dimensions**: 768
- **Optimized for**: CPU inference
- **Auto-download**: Model downloads automatically on first use

```python
from statement_extractor.database import CompanyEmbedder

embedder = CompanyEmbedder()
embedding = embedder.embed("Apple Inc")  # Returns 768-dim numpy array
```

---

### Troubleshooting

<span id="troubleshooting-db" />

**Database not found:**
```
Error: Database not found at ~/.cache/corp-extractor/entities.db
```
Run `corp-extractor db download` to fetch the pre-built database.

**sqlite-vec extension error:**
```
Error: no such module: vec0
```
The sqlite-vec extension should install automatically. If not: `pip install sqlite-vec`

**Memory issues with large dumps:**
```bash
# Import in smaller batches
corp-extractor db import-wikidata-dump --dump dump.bz2 --limit 10000 --skip-updates
# Then resume for more
corp-extractor db import-wikidata-dump --dump dump.bz2 --limit 10000 --skip-updates --resume
```

**Resume interrupted import:**
```bash
corp-extractor db import-wikidata-dump --dump dump.bz2 --resume
```
Progress is saved to `~/.cache/corp-extractor/wikidata-dump-progress.json`.
