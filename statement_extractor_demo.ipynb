{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Statement Extractor Demo\n",
        "\n",
        "This notebook demonstrates how to use the **corp-extractor** library to extract structured subject-predicate-object triples from unstructured text.\n",
        "\n",
        "**Features:**\n",
        "- Transform text into structured triples using T5-Gemma2\n",
        "- Entity type recognition (ORG, PERSON, GPE, etc.)\n",
        "- 5-stage extraction pipeline with pluggable components\n",
        "- Entity database for organization and person lookup\n",
        "- Document processing (URLs, PDFs)\n",
        "\n",
        "**Resources:**\n",
        "- [PyPI Package](https://pypi.org/project/corp-extractor/)\n",
        "- [GitHub Repository](https://github.com/corp-o-rate/statement-extractor)\n",
        "- [Hugging Face Model](https://huggingface.co/corp-o-rate/t5gemma2-statement-extractor)"
      ],
      "metadata": {
        "id": "intro_header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "Before running this notebook:\n",
        "\n",
        "1. **Use a GPU runtime**: Runtime → Change runtime type → T4 GPU\n",
        "2. **Accept the Gemma license**: Visit [google/gemma-3-12b-it-qat-q4_0-gguf](https://huggingface.co/google/gemma-3-12b-it-qat-q4_0-gguf) and accept the license agreement\n",
        "3. **Have a HuggingFace account**: You'll need to login below"
      ],
      "metadata": {
        "id": "setup_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_cell"
      },
      "outputs": [],
      "source": [
        "# Install the corp-extractor package\n",
        "!pip install -q corp-extractor\n",
        "\n",
        "# Verify installation\n",
        "import statement_extractor\n",
        "print(f\"Installed version: {statement_extractor.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to HuggingFace (required for gated models)\n",
        "# This will prompt you to enter your HuggingFace token\n",
        "# Get your token at: https://huggingface.co/settings/tokens\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "hf_login"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "gpu_check"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Simple Extraction\n",
        "\n",
        "The simplest way to use the library is with the `extract_statements` function."
      ],
      "metadata": {
        "id": "simple_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statement_extractor import extract_statements\n",
        "\n",
        "# Extract statements from text\n",
        "text = \"Apple Inc. announced a new iPhone today. Tim Cook presented the device at their Cupertino headquarters.\"\n",
        "\n",
        "result = extract_statements(text)\n",
        "\n",
        "# Display the extracted statements\n",
        "print(f\"Found {len(result)} statements:\\n\")\n",
        "for stmt in result:\n",
        "    print(f\"  Subject: {stmt.subject.text} ({stmt.subject.entity_type})\")\n",
        "    print(f\"  Predicate: {stmt.predicate}\")\n",
        "    print(f\"  Object: {stmt.object.text} ({stmt.object.entity_type})\")\n",
        "    print(f\"  Confidence: {stmt.confidence_score:.2f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "simple_extraction"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Formats\n",
        "\n",
        "You can also get results in different formats:"
      ],
      "metadata": {
        "id": "formats_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statement_extractor import (\n",
        "    extract_statements_as_dict,\n",
        "    extract_statements_as_json,\n",
        "    extract_statements_as_xml\n",
        ")\n",
        "\n",
        "text = \"Microsoft acquired Activision Blizzard for $68.7 billion.\"\n",
        "\n",
        "# Get as dictionary\n",
        "data = extract_statements_as_dict(text)\n",
        "print(\"As Dictionary:\")\n",
        "print(data)\n",
        "print()"
      ],
      "metadata": {
        "id": "format_dict"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get as JSON\n",
        "import json\n",
        "json_str = extract_statements_as_json(text)\n",
        "print(\"As JSON:\")\n",
        "print(json.dumps(json.loads(json_str), indent=2))"
      ],
      "metadata": {
        "id": "format_json"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get as XML\n",
        "xml_str = extract_statements_as_xml(text)\n",
        "print(\"As XML:\")\n",
        "print(xml_str)"
      ],
      "metadata": {
        "id": "format_xml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Full Extraction Pipeline\n",
        "\n",
        "For more comprehensive extraction, use the 5-stage pipeline:\n",
        "\n",
        "| Stage | Name | Description |\n",
        "|-------|------|-------------|\n",
        "| 1 | Splitting | Text → raw triples (T5-Gemma2) |\n",
        "| 2 | Extraction | Raw triples → typed statements (GLiNER2) |\n",
        "| 3 | Qualification | Add identifiers + canonical names |\n",
        "| 4 | Labeling | Add sentiment, relation type |\n",
        "| 5 | Taxonomy | Classify against taxonomies |"
      ],
      "metadata": {
        "id": "pipeline_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statement_extractor.pipeline import ExtractionPipeline, PipelineConfig\n",
        "\n",
        "# Create pipeline with default config\n",
        "pipeline = ExtractionPipeline()\n",
        "\n",
        "# Process text through all stages\n",
        "text = \"\"\"\n",
        "Amazon CEO Andy Jassy announced plans to invest $4 billion in AI infrastructure.\n",
        "The company will build new data centers in Virginia and Oregon.\n",
        "AWS, Amazon's cloud division, will lead the initiative.\n",
        "\"\"\"\n",
        "\n",
        "ctx = pipeline.process(text)\n",
        "\n",
        "print(f\"Pipeline completed. Found {len(ctx.labeled_statements)} labeled statements.\\n\")"
      ],
      "metadata": {
        "id": "pipeline_basic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore the labeled statements\n",
        "for i, labeled in enumerate(ctx.labeled_statements, 1):\n",
        "    stmt = labeled.statement\n",
        "    print(f\"Statement {i}:\")\n",
        "    print(f\"  {labeled.subject_fqn} → {stmt.predicate} → {labeled.object_fqn}\")\n",
        "\n",
        "    # Show labels\n",
        "    if labeled.labels:\n",
        "        print(f\"  Labels: {labeled.labels}\")\n",
        "\n",
        "    # Show taxonomy classifications\n",
        "    if labeled.taxonomy_results:\n",
        "        top_topics = sorted(labeled.taxonomy_results, key=lambda x: x.score, reverse=True)[:2]\n",
        "        print(f\"  Topics: {[t.topic for t in top_topics]}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "pipeline_explore"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pipeline Configuration\n",
        "\n",
        "You can customize which stages run and which plugins are enabled:"
      ],
      "metadata": {
        "id": "pipeline_config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run only stages 1-3 (skip labeling and taxonomy)\n",
        "config = PipelineConfig(\n",
        "    enabled_stages={1, 2, 3},\n",
        ")\n",
        "\n",
        "pipeline = ExtractionPipeline(config)\n",
        "ctx = pipeline.process(\"Google announced Gemini 2.0 at their Mountain View campus.\")\n",
        "\n",
        "print(f\"Stages 1-3 only: {len(ctx.statements)} statements extracted\")\n",
        "for stmt in ctx.statements:\n",
        "    print(f\"  {stmt.subject.text} → {stmt.predicate} → {stmt.object.text}\")"
      ],
      "metadata": {
        "id": "pipeline_config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List available plugins\n",
        "from statement_extractor.pipeline import PluginRegistry\n",
        "\n",
        "print(\"Available plugins by stage:\")\n",
        "print(f\"  Splitters: {list(PluginRegistry._splitters.keys())}\")\n",
        "print(f\"  Extractors: {list(PluginRegistry._extractors.keys())}\")\n",
        "print(f\"  Qualifiers: {list(PluginRegistry._qualifiers.keys())}\")\n",
        "print(f\"  Labelers: {list(PluginRegistry._labelers.keys())}\")\n",
        "print(f\"  Taxonomy: {list(PluginRegistry._taxonomy_classifiers.keys())}\")"
      ],
      "metadata": {
        "id": "list_plugins"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Entity Database\n",
        "\n",
        "The library includes an entity database for organization and person lookup. This enables entity qualification with canonical IDs."
      ],
      "metadata": {
        "id": "database_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the entity database (lite version, ~500MB)\n",
        "!corp-extractor db download"
      ],
      "metadata": {
        "id": "db_download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check database status\n",
        "!corp-extractor db status"
      ],
      "metadata": {
        "id": "db_status"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for organizations\n",
        "from statement_extractor.database import OrganizationDatabase\n",
        "\n",
        "db = OrganizationDatabase()\n",
        "\n",
        "# Search for Microsoft\n",
        "results = db.search(\"Microsoft\", limit=5)\n",
        "print(\"Search results for 'Microsoft':\")\n",
        "for match in results:\n",
        "    print(f\"  {match.record.name} (score: {match.score:.3f})\")\n",
        "    print(f\"    Type: {match.record.entity_type}\")\n",
        "    print(f\"    Source: {match.record.source}\")\n",
        "    if match.record.lei:\n",
        "        print(f\"    LEI: {match.record.lei}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "db_search_org"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Search for people\n",
        "from statement_extractor.database import PersonDatabase\n",
        "\n",
        "people_db = PersonDatabase()\n",
        "\n",
        "results = people_db.search(\"Elon Musk\", limit=5)\n",
        "print(\"Search results for 'Elon Musk':\")\n",
        "for match in results:\n",
        "    print(f\"  {match.record.name} (score: {match.score:.3f})\")\n",
        "    print(f\"    Type: {match.record.person_type}\")\n",
        "    print(f\"    Role: {match.record.role}\")\n",
        "    if match.record.org_name:\n",
        "        print(f\"    Organization: {match.record.org_name}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "db_search_people"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Document Processing\n",
        "\n",
        "Process entire documents including URLs and PDFs:"
      ],
      "metadata": {
        "id": "document_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a URL (example with a news article)\n",
        "# Note: This requires the document to be accessible\n",
        "\n",
        "from statement_extractor.document import DocumentPipeline\n",
        "\n",
        "doc_pipeline = DocumentPipeline()\n",
        "\n",
        "# Process from a text file or string\n",
        "sample_doc = \"\"\"\n",
        "Tesla announced record quarterly deliveries of 500,000 vehicles.\n",
        "CEO Elon Musk attributed the growth to strong demand in China.\n",
        "The company's Shanghai Gigafactory produced 250,000 units.\n",
        "Tesla stock rose 5% following the announcement.\n",
        "\"\"\"\n",
        "\n",
        "result = doc_pipeline.process_text(sample_doc)\n",
        "\n",
        "print(f\"Document processing found {len(result.statements)} statements:\")\n",
        "for stmt in result.statements[:5]:  # Show first 5\n",
        "    print(f\"  {stmt.subject.text} → {stmt.predicate} → {stmt.object.text}\")"
      ],
      "metadata": {
        "id": "document_process"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. CLI Usage\n",
        "\n",
        "The library also provides a command-line interface:"
      ],
      "metadata": {
        "id": "cli_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple extraction\n",
        "!corp-extractor split \"Apple released the iPhone 16 with new AI features.\""
      ],
      "metadata": {
        "id": "cli_split"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full pipeline with verbose output\n",
        "!corp-extractor pipeline \"Amazon acquired Whole Foods for \\$13.7 billion.\" -v"
      ],
      "metadata": {
        "id": "cli_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List available plugins\n",
        "!corp-extractor plugins list"
      ],
      "metadata": {
        "id": "cli_plugins"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Advanced: Custom Extraction Options\n",
        "\n",
        "Fine-tune extraction with custom options:"
      ],
      "metadata": {
        "id": "advanced_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statement_extractor import StatementExtractor, ExtractionOptions\n",
        "\n",
        "# Create extractor with custom options\n",
        "options = ExtractionOptions(\n",
        "    num_beams=5,           # Number of beams for diverse beam search\n",
        "    num_beam_groups=5,     # Number of beam groups\n",
        "    diversity_penalty=0.5, # Penalty for similar beams\n",
        "    max_new_tokens=512,    # Maximum output length\n",
        ")\n",
        "\n",
        "extractor = StatementExtractor(options=options)\n",
        "\n",
        "text = \"NVIDIA reported $26 billion in revenue, driven by AI chip demand from Microsoft and Google.\"\n",
        "result = extractor.extract(text)\n",
        "\n",
        "print(f\"Extracted {len(result)} statements with custom options:\")\n",
        "for stmt in result:\n",
        "    print(f\"  {stmt.subject.text} → {stmt.predicate} → {stmt.object.text}\")"
      ],
      "metadata": {
        "id": "advanced_options"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. **Simple extraction** with `extract_statements()`\n",
        "2. **Multiple output formats** (dict, JSON, XML)\n",
        "3. **Full pipeline** with 5 stages of processing\n",
        "4. **Pipeline configuration** to enable/disable stages\n",
        "5. **Entity database** for organization and person lookup\n",
        "6. **Document processing** for longer texts\n",
        "7. **CLI commands** for terminal usage\n",
        "8. **Custom extraction options** for fine-tuning\n",
        "\n",
        "For more information, see the [documentation](https://github.com/corp-o-rate/statement-extractor)."
      ],
      "metadata": {
        "id": "summary_header"
      }
    }
  ]
}
